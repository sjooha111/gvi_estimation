import os, gzip, torch, numpy as np, pandas as pd
from model.Revit import ReViT

# ====== ì„¤ì • ======
USE_NORMALIZATION = True  # <- ì •ê·œí™” ë„ê³  ì‹¶ìœ¼ë©´ Falseë¡œ ë°”ê¾¸ê±°ë‚˜ ê´€ë ¨ ë¶€ë¶„ ì‚­ì œ

# --- Config (ê°„ì†Œí™” ë²„ì „) ---
class Config:
    class DATA: crop_size = 224
    class MODEL:
        num_classes = 1
        dropout_rate = 0.1
        head_act = None
    class ReViT:
        mode = "conv"
        pool_first = False
        patch_kernel = [16, 16]
        patch_stride = [16, 16]
        patch_padding = [0, 0]
        embed_dim = 768
        num_heads = 12
        mlp_ratio = 4
        qkv_bias = True
        drop_path = 0.2
        depth = 12
        dim_mul = []
        head_mul = []
        pool_qkv_kernel = []
        pool_kv_stride_adaptive = []
        pool_q_stride = []
        zero_decay_pos = False
        use_abs_pos = True
        use_rel_pos = False
        rel_pos_zero_init = False
        residual_pooling = False
        dim_mul_in_att = False
        alpha = True
        visualize = True
        cls_embed_on = False


def load_npy_gz(path):
    """ì••ì¶•ëœ npy.gz íŒŒì¼ì„ ì½ì–´ì„œ torch tensorë¡œ ë³€í™˜"""
    with gzip.open(path, "rb") as f:
        arr = np.load(f)
    return torch.tensor(arr, dtype=torch.float32).unsqueeze(0)  # [1,C,H,W]


def load_minmax_stats(stats_path, device):
    """
    normalization_stats.txtì—ì„œ Overall Max/Min ê°’ì„ ì½ì–´ì„œ
    [1, C, 1, 1] shapeì˜ tensorë¡œ ë°˜í™˜
    ê¸°ëŒ€ í˜•ì‹:
        Overall Max:
        1.22,2.15, ...
        Overall Min:
        0.002,0.002, ...
    í˜¹ì€ í•œ ì¤„ì— ì½œë¡  ë’¤ì— ë°”ë¡œ ê°’ë“¤ì´ ìˆì–´ë„ ë™ì‘í•˜ë„ë¡ ì²˜ë¦¬
    """
    if not os.path.exists(stats_path):
        print(f"[WARN] normalization stats file not found: {stats_path}")
        return None, None

    with open(stats_path, "r", encoding="utf-8") as f:
        lines = [l.strip() for l in f.readlines() if l.strip()]

    max_vals = None
    min_vals = None

    for line in lines:
        lower = line.lower()
        if "overall max" in lower:
            # "Overall Max:" ë¼ì¸ì¼ ìˆ˜ë„ ìˆê³ , "Overall Max: 1.22,..." í•œ ì¤„ì¼ ìˆ˜ë„ ìˆìŒ
            if ":" in line:
                after = line.split(":", 1)[1].strip()
                if after:
                    max_vals = [float(x) for x in after.split(",")]
            # ê°’ì´ ë‹¤ìŒ ì¤„ì— ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (ì˜ˆ: ë‹¤ìŒ ì¤„ì´ ìˆ«ì ë¦¬ìŠ¤íŠ¸)
        elif (max_vals is None and
              all(c.isdigit() or c in ".,- " for c in line) and
              "overall" not in lower):
            # ìˆ«ì ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ë³´ì´ë©´ max ë˜ëŠ” minì¼ ìˆ˜ ìˆìŒ (ì•ì—ì„œ max ë¼ì¸ì„ ë³¸ í›„ì¼ ê²ƒ)
            # í•˜ì§€ë§Œ ì´ê±´ ì•„ë˜ì—ì„œ ë‹¤ì‹œ ì²˜ë¦¬í•˜ë¯€ë¡œ pass
            pass

    # ìœ„ì—ì„œ maxë¥¼ ëª» ì½ì€ ê²½ìš°, "Overall Max:" ë‹¤ìŒ ì¤„ ë°©ì‹ ì²˜ë¦¬
    if max_vals is None:
        for i, line in enumerate(lines):
            if "overall max" in line.lower() and i + 1 < len(lines):
                max_vals = [float(x) for x in lines[i + 1].split(",")]
                break

    # Minë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    for line in lines:
        lower = line.lower()
        if "overall min" in lower:
            if ":" in line:
                after = line.split(":", 1)[1].strip()
                if after:
                    min_vals = [float(x) for x in after.split(",")]

    if min_vals is None:
        for i, line in enumerate(lines):
            if "overall min" in line.lower() and i + 1 < len(lines):
                min_vals = [float(x) for x in lines[i + 1].split(",")]
                break

    if max_vals is None or min_vals is None:
        print("[WARN] Failed to parse Overall Max/Min from stats file.")
        return None, None

    if len(max_vals) != len(min_vals):
        print("[WARN] len(max_vals) != len(min_vals). Check stats file.")
        return None, None

    max_t = torch.tensor(max_vals, dtype=torch.float32, device=device).view(1, -1, 1, 1)
    min_t = torch.tensor(min_vals, dtype=torch.float32, device=device).view(1, -1, 1, 1)

    print(f"[INFO] Loaded normalization stats for {len(max_vals)} channels from {stats_path}")
    print(f"       Max (first 5): {max_vals[:5]}")
    print(f"       Min (first 5): {min_vals[:5]}")
    return min_t, max_t

def normalize_input(x, ch_min, ch_max):
    """
    x: [B, C, H, W]
    ch_min, ch_max: [1, K, 1, 1]  (K <= C ë¼ê³  ê°€ì •)
    ì•ì˜ Kê°œ ì±„ë„ë§Œ (x - min) / (max - min) ìœ¼ë¡œ ì •ê·œí™”í•˜ê³ ,
    ë‚˜ë¨¸ì§€ C-K ì±„ë„ì€ ì›ë³¸ ê·¸ëŒ€ë¡œ ë‘”ë‹¤.
    """
    if ch_min is None or ch_max is None:
        return x

    B, C, H, W = x.shape
    K = ch_min.shape[1]

    if K > C:
        print(
            f"[WARN] Stats have {K} channels but input has only {C} channels. "
            f"Skipping normalization."
        )
        return x

    # ë³µì‚¬ë³¸ ë§Œë“¤ì–´ì„œ ì• Kì±„ë„ë§Œ ìˆ˜ì •
    x_norm = x.clone()

    denom = ch_max[:, :K] - ch_min[:, :K]
    denom = torch.where(denom == 0, torch.ones_like(denom), denom)

    x_k = x[:, :K]  # ì• Kê°œ ì±„ë„
    x_k = (x_k - ch_min[:, :K]) / denom
    x_k = torch.clamp(x_k, 0.0, 1.0)

    x_norm[:, :K] = x_k

    if C > K:
        print(f"[INFO] Normalized first {K} channels out of {C}. "
              f"Remaining {C-K} channels are left unchanged.")

    return x_norm

def main():
    repo_root = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(repo_root, "data", "RS_sample")
    ckpt_path = os.path.join(repo_root, "checkpoint", "Revit_checkpoint.bin")
    # truth_csv = os.path.join(repo_root, "data", "dataset.csv")
    truth_csv = os.path.join(repo_root, "data", "dataset_mapo.csv")
    stats_path = os.path.join(repo_root, "normalization_stats.txt")  # <- ì—¬ê¸°ì„œ íŒŒì¼ ì‚¬ìš©
    # output_dir = os.path.join(repo_root, "output")
    output_dir = os.path.join(repo_root, "output_mapo")
    os.makedirs(output_dir, exist_ok=True)
    out_csv = os.path.join(output_dir, "prediction_with_truth.csv")
    metrics_path = os.path.join(output_dir, "metrics_nm.txt")


    # --- Load dataset.csv (ëª¨ë“  ì»¬ëŸ¼ ìœ ì§€) ---
    df_truth = pd.read_csv(truth_csv, encoding="cp949")

    # --- Load model ---
    cfg = Config()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = ReViT(cfg).to(device)
    state = torch.load(ckpt_path, map_location=device)
    model.load_state_dict(state["state_dict"] if "state_dict" in state else state)
    model.eval()

    # --- Load normalization stats (optional) ---
    ch_min, ch_max = None, None
    if USE_NORMALIZATION:
        ch_min, ch_max = load_minmax_stats(stats_path, device)
        if ch_min is None or ch_max is None:
            print("[WARN] Normalization disabled because stats could not be loaded.")
            # ì‹¤íŒ¨í–ˆìœ¼ë©´ ê·¸ëƒ¥ ì •ê·œí™” ì—†ì´ ì§„í–‰
            # (ì›í•˜ë©´ ì—¬ê¸°ì„œ í”„ë¡œê·¸ë¨ ì¢…ë£Œí•´ë„ ë¨)
            # exit(1)

    gx = torch.tensor([[0.5, 0.5]], dtype=torch.float32).to(device)
    preds = []

    for fname in sorted(os.listdir(data_dir)):
        if not fname.endswith(".npy.gz"):
            continue

        # íŒŒì¼ëª…ì—ì„œ sid ì¶”ì¶œ
        sid = int(os.path.splitext(fname.split(".")[0])[0])
        x = load_npy_gz(os.path.join(data_dir, fname)).to(device)  # [1, C, H, W]

        # --- ì—¬ê¸°ì„œ normalization ì ìš© ---
        if USE_NORMALIZATION and ch_min is not None and ch_max is not None:
            x = normalize_input(x, ch_min, ch_max)

        with torch.no_grad():
            pred = model(x, gx).squeeze().item()

        preds.append((sid, pred))

        # ì •ë‹µê°’ì´ ìˆìœ¼ë©´ ì¶œë ¥
        row = df_truth[df_truth["sid"] == sid]
        if not row.empty:
            gvi_true = row.iloc[0]["gvi"]
            print(f"{fname} (sid={sid}) â†’ Pred: {pred:.4f}, Truth: {gvi_true:.4f}")
        else:
            print(f"{fname} (sid={sid}) â†’ Pred: {pred:.4f}, Truth: ì—†ìŒ")

    # --- DataFrame ë³‘í•© ---
    df_pred = pd.DataFrame(preds, columns=["sid", "predicted_GVI"])
    df_merge = pd.merge(df_truth, df_pred, on="sid", how="left")

    # === ì„±ëŠ¥ í‰ê°€ (ì •ë‹µ gviì™€ ì˜ˆì¸¡ê°’ì´ ëª¨ë‘ ìˆëŠ” ìƒ˜í”Œë§Œ ì‚¬ìš©) ===
    valid = df_merge.dropna(subset=["gvi", "predicted_GVI"])
    if len(valid) > 0:
        y_true = valid["gvi"].to_numpy()
        y_pred = valid["predicted_GVI"].to_numpy()

        mae = np.mean(np.abs(y_true - y_pred))
        mse = np.mean((y_true - y_pred) ** 2)
        rmse = np.sqrt(mse)
        # ë¶„ì‚°ì´ 0ì¸ ê²½ìš°(ëª¨ë“  ì •ë‹µì´ ê°™ì€ ê°’) R^2 ì •ì˜ ë¶ˆê°€ â†’ np.nan ì²˜ë¦¬
        denom = np.sum((y_true - y_true.mean()) ** 2)
        if denom == 0:
            r2 = np.nan
        else:
            r2 = 1 - np.sum((y_true - y_pred) ** 2) / denom

        print("\n=== Evaluation on test data (samples with ground-truth GVI) ===")
        print(f"#Samples: {len(valid)}")
        print(f"MAE : {mae:.4f}")
        print(f"RMSE: {rmse:.4f}")
        print(f"MSE : {mse:.4f}")
        print(f"R^2 : {r2:.4f}" if not np.isnan(r2) else "R^2 : NaN (constant ground truth)")

        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œë„ ì €ì¥
        with open(metrics_path, "w", encoding="utf-8") as f:
            f.write("Evaluation on test data (samples with ground-truth GVI)\n")
            f.write(f"#Samples: {len(valid)}\n")
            f.write(f"MAE : {mae:.6f}\n")
            f.write(f"RMSE: {rmse:.6f}\n")
            f.write(f"MSE : {mse:.6f}\n")
            f.write(f"R^2 : {r2:.6f}\n" if not np.isnan(r2) else "R^2 : NaN (constant ground truth)\n")
        print(f"\nğŸ“„ Saved metrics to: {metrics_path}")
    else:
        print("\nâš  í‰ê°€í•  ìˆ˜ ìˆëŠ” (gvi+ì˜ˆì¸¡) ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")

    # --- Save (ì˜ˆì¸¡ í¬í•¨ ì „ì²´ í…Œì´ë¸”) ---
    df_merge.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"\nâœ… Saved results (all dataset columns + predicted_GVI) to:\n{out_csv}")


if __name__ == "__main__":
    main()
